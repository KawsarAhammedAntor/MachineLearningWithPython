{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Workshop\n",
    "\n",
    "## Introduction\n",
    "Welcome to this comprehensive workshop on Logistic Regression! In this 2-hour session, we'll explore both the theoretical foundations and practical applications of logistic regression, one of the fundamental algorithms in machine learning for classification tasks.\n",
    "\n",
    "### Workshop Outline\n",
    "1. Understanding Logistic Regression\n",
    "   - What is Logistic Regression?\n",
    "   - Linear vs. Logistic Regression\n",
    "   - The Sigmoid Function\n",
    "2. Mathematics Behind Logistic Regression\n",
    "   - Probability and Odds\n",
    "   - Log Odds (Logit)\n",
    "   - Cost Function\n",
    "3. Implementation\n",
    "   - Data Preparation\n",
    "   - Model Training\n",
    "   - Making Predictions\n",
    "4. Model Evaluation\n",
    "   - Confusion Matrix\n",
    "   - Accuracy, Precision, Recall\n",
    "   - ROC Curve and AUC\n",
    "5. Practical Example\n",
    "   - Real-world Dataset\n",
    "   - Complete Implementation\n",
    "\n",
    "Let's begin by importing the necessary libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Logistic Regression\n",
    "\n",
    "### What is Logistic Regression?\n",
    "Logistic regression is a statistical method for predicting binary outcomes. Despite its name, it's used for classification rather than regression. Some common examples include:\n",
    "- Email spam detection (spam/not spam)\n",
    "- Disease diagnosis (present/absent)\n",
    "- Customer churn prediction (will churn/won't churn)\n",
    "\n",
    "### Linear vs. Logistic Regression\n",
    "While linear regression predicts continuous values, logistic regression predicts the probability of an instance belonging to a particular class. The key difference lies in the output:\n",
    "- Linear Regression: y = mx + b (can be any real number)\n",
    "- Logistic Regression: P(y=1) = 1 / (1 + e^-(mx + b)) (bounded between 0 and 1)\n",
    "\n",
    "Let's visualize this difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate sample data\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y_linear = 0.5 * x + 2\n",
    "y_logistic = 1 / (1 + np.exp(-0.5 * x))\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot linear regression\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_linear)\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot logistic regression\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_logistic)\n",
    "plt.title('Logistic Regression (Sigmoid)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid Function\n",
    "The sigmoid function (also called the logistic function) is the key component that transforms linear predictions into probabilities:\n",
    "\n",
    "σ(z) = 1 / (1 + e^-z)\n",
    "\n",
    "Properties of the sigmoid function:\n",
    "1. Output is always between 0 and 1\n",
    "2. S-shaped curve\n",
    "3. Differentiable (important for optimization)\n",
    "\n",
    "## 2. Mathematics Behind Logistic Regression\n",
    "\n",
    "### Probability and Odds\n",
    "- Probability (P): The likelihood of an event occurring (0 ≤ P ≤ 1)\n",
    "- Odds: The ratio of probability of success to probability of failure [P/(1-P)]\n",
    "\n",
    "### Log Odds (Logit)\n",
    "The logit function is the natural logarithm of the odds:\n",
    "\n",
    "logit(P) = ln(P/(1-P)) = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
    "\n",
    "### Cost Function\n",
    "The cost function for logistic regression is the negative log-likelihood:\n",
    "\n",
    "J(θ) = -1/m Σ [y * log(h(x)) + (1-y) * log(1-h(x))]\n",
    "\n",
    "where:\n",
    "- m is the number of training examples\n",
    "- y is the actual class\n",
    "- h(x) is the predicted probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "Let's implement logistic regression on a simple dataset. We'll create a synthetic dataset for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(size=(100, 2))\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Let's evaluate our model using various metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print classification report\n",
    "print('Classification Report:\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example\n",
    "\n",
    "Now let's work with a real-world dataset. We'll use the famous Breast Cancer Wisconsin dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the data\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report for Breast Cancer Prediction:\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(15, 6))\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': data.feature_names,\n",
    "    'Importance': abs(model.coef_[0])\n",
    "})\n",
    "importance = importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.bar(importance['Feature'][:10], importance['Importance'][:10])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we've covered:\n",
    "1. The fundamental concepts of logistic regression\n",
    "2. The mathematics behind the algorithm\n",
    "3. Implementation using scikit-learn\n",
    "4. Model evaluation techniques\n",
    "5. A real-world application\n",
    "\n",
    "### Key Takeaways\n",
    "- Logistic regression is best suited for binary classification problems\n",
    "- Feature scaling is important for better model performance\n",
    "- Multiple metrics should be considered when evaluating the model\n",
    "- The algorithm can provide insights into feature importance\n",
    "\n",
    "### Next Steps\n",
    "- Try implementing multiclass logistic regression\n",
    "- Experiment with different regularization parameters\n",
    "- Apply the model to your own classification problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
